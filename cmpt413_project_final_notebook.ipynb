{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial Logistic Regression"
      ],
      "metadata": {
        "id": "1tkZEcx61dvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_ozAH34fRsTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the neccessary libraries:\n",
        "\n",
        "1.   numpy and pandas for processing and transforming data\n",
        "2.   From sklearn LogisticRegression for the model, TfidfVectorizer for turning input text to vector, cross_val_score for evaluating the model, and train_test_split for splitting the input data to train and test\n",
        "3.   hstack from scipy to stack multiple vectors on top of each other\n",
        "\n"
      ],
      "metadata": {
        "id": "Ak8OpVXo1hrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "hYFrhMsaN15d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data, and then split it to train and test using train_test_split. The test split is 70-30 and random state is set to 999 to match the other two algorithms."
      ],
      "metadata": {
        "id": "re9v-AkfTQGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "data = pd.read_csv('comments-rated.csv').fillna(' ')\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=999)\n",
        "\n",
        "train = train_data['comment_text']\n",
        "test = test_data['comment_text']\n",
        "all = pd.concat([train, test])"
      ],
      "metadata": {
        "id": "T6697FwFPNrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_TfidfVectorizer turns each word in the input to a vector using the TF-IDF method. Description of each paramater:\n",
        "\n",
        "\n",
        "*   strip_accents: remove accents and perform other character normalization during the preprocessing step based on 'unicode'\n",
        "*   analyzer: set to word so it will apply TF-IDF on words and not characters\n",
        "*   token_pattern: regular expression denoting what constitutes a “token” \n",
        "*   stop_words: remove stop words in the pre-processing stages. 'english' is the only supported value, and it will remove words that do not give any useful information to the model\n",
        "*   ngram_range: covers how many words in each sentence to apply the TF-IDF method to vectorize. It set is (1, 1) meaning the vectorizer will go over the unigramsword_TfidfVectorizer\n",
        "*   max_features: an upper limit on the number of features\n"
      ],
      "metadata": {
        "id": "9nMJ2_ULTQkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_TfidfVectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1),\n",
        "    max_features=10000)\n",
        "word_TfidfVectorizer.fit(all)\n",
        "train_word_features = word_TfidfVectorizer.transform(train)\n",
        "test_word_features = word_TfidfVectorizer.transform(test)"
      ],
      "metadata": {
        "id": "PirEbs3NPNzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "char_TfidfVectorizer applies the TF-IDF method to the character of each word in order to include more information regarding the input. Description of each paramter: \n",
        "\n",
        "\n",
        "*   strip_accents: remove accents and perform other character normalization during the preprocessing step based on 'unicode'\n",
        "\n",
        "*   analyzer: set to char so it will apply TF-IDF on characters of each word and not the words themselves\n",
        "*   ngram_range: covers how many characters in each word to apply the TF-IDF method to vectorize. It set is (2, 6) meaning the vectorizer will go over every two, three,... up to six characters in each word and embed their information in the input vector\n",
        "*   max_features: an upper limit on the number of features"
      ],
      "metadata": {
        "id": "uad2eZJoTQ73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_TfidfVectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='char',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=50000)\n",
        "char_TfidfVectorizer.fit(all)\n",
        "train_char_features = char_TfidfVectorizer.transform(train)\n",
        "test_char_features = char_TfidfVectorizer.transform(test)"
      ],
      "metadata": {
        "id": "D5DAJOMcPN5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train features to input to LR will include word embeddings calculated using TF-IDF and n-gram character representations (and so will the test features)"
      ],
      "metadata": {
        "id": "cEztFT6fTRWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = hstack([train_char_features, train_word_features])\n",
        "test_features = hstack([test_char_features, test_word_features])"
      ],
      "metadata": {
        "id": "RPF3ThyfPN7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a logistic regression model for each class, train the model using the train data. Then, using cross_val_score validate the model on the test data over the evaluation metrics (Precision, Recall, F1, ROC_AUC)"
      ],
      "metadata": {
        "id": "hm7Zij7eTSVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for class_name in class_names:\n",
        "    train_target = train_data[class_name]\n",
        "    test_target = test_data[class_name]\n",
        "    start = time.time()\n",
        "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
        "    classifier.fit(train_features, train_target)\n",
        "    end = time.time()\n",
        "    print(\"{} took {} seconds\".format(class_name, end - start))\n",
        "    p_score = np.mean(cross_val_score(classifier, test_features, test_target, cv=3, scoring='precision'))\n",
        "    r_score = np.mean(cross_val_score(classifier, test_features, test_target, cv=3, scoring='recall'))\n",
        "    f1_score = np.mean(cross_val_score(classifier, test_features, test_target, cv=3, scoring='f1'))\n",
        "    roc_auc_score = np.mean(cross_val_score(classifier, test_features, test_target, cv=3, scoring='roc_auc'))\n",
        "    print(\"{} p_score {} r_score {} f1_score {} roc_auc_score {}\".format(class_name, p_score, r_score, f1_score, roc_auc_score))"
      ],
      "metadata": {
        "id": "TLMlSuBXPN-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6xW-gJ11b7t"
      },
      "source": [
        "# 1-D Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n-Sv9yTKRoTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJLj2t3Y1b7w"
      },
      "source": [
        "Import the necessary libraries. Pandas is used for processing and transforming data; TensorFlow and Keras provide the main ML functionality and model APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3erRKT191b7x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kMse3uD1b7x"
      },
      "source": [
        "Read in data and prepare for processing by isolating X (comment text) and Y (binary classification by label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJrWJSz51b7y"
      },
      "outputs": [],
      "source": [
        "df_comments = pd.read_csv('comments-rated.csv')\n",
        "cols_to_drop = [0, 2, 3, 4, 5, 6, 7]\n",
        "df_comments.drop(df_comments.columns[cols_to_drop],axis=1,inplace=True)\n",
        "df_comments = df_comments\n",
        "\n",
        "df_labels = pd.read_csv('comments-rated.csv')\n",
        "cols_to_drop = [0,1]\n",
        "df_labels.drop(df_labels.columns[cols_to_drop],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-GPpAdl1b7y"
      },
      "source": [
        "Tokenize input text and transform into feature vectors to be machine readable format using Keras' text preprocessing API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAPuCpvz1b7y"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "maxlen = 250\n",
        "max_words = 5000\n",
        "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
        "tokenizer.fit_on_texts(df_comments.comment_text)\n",
        "\n",
        "def get_features(text_series):\n",
        "    sequences = tokenizer.texts_to_sequences(text_series)\n",
        "    return pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "x = get_features(df_comments.comment_text)\n",
        "y = tf.convert_to_tensor(df_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fEGkRV71b7z"
      },
      "source": [
        "Generate 70/30 train/test split of data using standardized random_state=999."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY438fqI1b70"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, df_labels, test_size=0.3, random_state=999)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR3lPuKj1b71"
      },
      "source": [
        "Build and compile a 1-Dimensional CNN with 6 dense layers (one for each class). Filter length, embedding dimension, and dropout hyperparameters are specified (implemented levels shown resulting from experimentation and tuning on dataset). Evaluation metrics are also specified during model compilation.\n",
        "\n",
        "A sequential model is used to stack layers in order. The input is represented in the form of a word embedding. The dropout layer helps prevent overfitting (regularization) by randomly setting input units to 0. The Conv1D layer is the temporal convolutional layer with the provided activation function. Max-over-time pooling reduces the data by downsampling the input by taking the maximum value over every window. And, Dense and Activation layers represent the final connected neural network layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vuv9zSC01b71"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "filter_length = 300\n",
        "embedding_dim = 20\n",
        "dropout = 0.1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim))\n",
        "model.add(Dropout(dropout))\n",
        "model.add(Conv1D(filter_length, 3, padding='same', activation='relu', strides=1))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(6))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\n",
        "        tf.keras.metrics.AUC(curve=\"ROC\", name=\"ROC_AUC\"), \n",
        "        tf.keras.metrics.Precision(name=\"P\"), \n",
        "        tf.keras.metrics.Recall(name=\"R\")\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIhoCfbv1b71"
      },
      "source": [
        "Fit model on training data, evaluate on testing data, and output results to file. This code loads the model written from this iteration of training which is saved in the same directory as the code itself. An example, already trained, model is located in source.zip/CNN/output/model-conv1d.h5 for easy loading and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdkKN57v1b72"
      },
      "outputs": [],
      "source": [
        "with open('output' + str(max_words) + '_' + str(dropout) + '_' + str(embedding_dim) + '.txt', 'w') as f:\n",
        "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "    callbacks = [\n",
        "        ReduceLROnPlateau(), \n",
        "        EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50),\n",
        "        ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        epochs=20,\n",
        "                        batch_size=32,\n",
        "                        validation_split=0.1,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "    cnn_model = keras.models.load_model('model-conv1d.h5')\n",
        "    metrics = cnn_model.evaluate(x_test, y_test, return_dict=True)\n",
        "    f.writelines(str(metrics))\n",
        "    fscore = 2 * (metrics['P'] * metrics['R']) / (metrics['P'] + metrics['R'])\n",
        "    f.writelines('\\n')\n",
        "    f.writelines('F-Score: ' + str(fscore))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM RNN"
      ],
      "metadata": {
        "id": "tHFCyfdZXWO1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcGq1dF0iWju"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEXu-mePiK9T"
      },
      "source": [
        "import numpy as np, pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "from string import ascii_lowercase\n",
        "from tqdm import tqdm_notebook\n",
        "import itertools\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "!pip install talos\n",
        "import talos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8KPqb7_vC59"
      },
      "source": [
        "## Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Z6cXAZiaAY"
      },
      "source": [
        "train=pd.read_csv('comments-rated.csv')     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8hAcigh3Sh3"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYZ93LlRbwRB"
      },
      "source": [
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "y = train[labels].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWRHguXxOF_"
      },
      "source": [
        "###Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z-4hq4jyhBw"
      },
      "source": [
        "#### Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4k-vI8DbqiP"
      },
      "source": [
        "* Removing Characters in between Text\n",
        "* Removing Repeated Characters\n",
        "* Converting data to lower-case\n",
        "* Removing Numbers from the data\n",
        "* Remove Punctuation\n",
        "* Remove Whitespaces\n",
        "* Removing spaces in between words\n",
        "* Removing \"\\n\"\n",
        "* Remove Non-english characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jglnA7LEbpoA"
      },
      "source": [
        "RE_PATTERNS = {\n",
        "    ' american ':\n",
        "        [\n",
        "            'amerikan'\n",
        "        ],\n",
        "\n",
        "    ' adolf ':\n",
        "        [\n",
        "            'adolf'\n",
        "        ],\n",
        "\n",
        "\n",
        "    ' hitler ':\n",
        "        [\n",
        "            'hitler'\n",
        "        ],\n",
        "\n",
        "    ' fuck':\n",
        "        [\n",
        "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
        "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
        "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
        "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
        "            'feck ', ' fux ', 'f\\*\\*', 'f**k','fu*k',\n",
        "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
        "        ],\n",
        "\n",
        "    ' ass ':\n",
        "        [\n",
        "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$',\n",
        "            '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
        "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s','a55', '@$$'\n",
        "        ],\n",
        "\n",
        "    ' ass hole ':\n",
        "        [\n",
        "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'a**hole'\n",
        "        ],\n",
        "\n",
        "    ' bitch ':\n",
        "        [\n",
        "            'b[w]*i[t]*ch', 'b!tch',\n",
        "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
        "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h', 'b!tch', 'bi+ch', 'l3itch'\n",
        "        ],\n",
        "\n",
        "    ' bastard ':\n",
        "        [\n",
        "            'ba[s|z]+t[e|a]+rd'\n",
        "        ],\n",
        "\n",
        "    ' trans gender':\n",
        "        [\n",
        "            'transgender'\n",
        "        ],\n",
        "\n",
        "    ' gay ':\n",
        "        [\n",
        "            'gay'\n",
        "        ],\n",
        "\n",
        "    ' cock ':\n",
        "        [\n",
        "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
        "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
        "        ],\n",
        "\n",
        "    ' dick ':\n",
        "        [\n",
        "            ' dick[^aeiou]', 'deek', 'd i c k', 'dik'\n",
        "        ],\n",
        "\n",
        "    ' suck ':\n",
        "        [\n",
        "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
        "        ],\n",
        "\n",
        "    ' cunt ':\n",
        "        [\n",
        "            'cunt', 'c u n t'\n",
        "        ],\n",
        "\n",
        "    ' bull shit ':\n",
        "        [\n",
        "            'bullsh\\*t', 'bull\\$hit'\n",
        "        ],\n",
        "\n",
        "    ' homo sex ual':\n",
        "        [\n",
        "            'homosexual'\n",
        "        ],\n",
        "\n",
        "    ' jerk ':\n",
        "        [\n",
        "            'jerk'\n",
        "        ],\n",
        "\n",
        "    ' idiot ':\n",
        "        [\n",
        "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots'\n",
        "                                                                                      'i d i o t'\n",
        "        ],\n",
        "\n",
        "    ' dumb ':\n",
        "        [\n",
        "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
        "        ],\n",
        "\n",
        "    ' shit ':\n",
        "        [\n",
        "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', '$h1t'\n",
        "        ],\n",
        "\n",
        "    ' shit hole ':\n",
        "        [\n",
        "            'shythole'\n",
        "        ],\n",
        "\n",
        "    ' retard ':\n",
        "        [\n",
        "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
        "        ],\n",
        "\n",
        "    ' rape ':\n",
        "        [\n",
        "            ' raped'\n",
        "        ],\n",
        "\n",
        "    ' dumb ass':\n",
        "        [\n",
        "            'dumbass', 'dubass'\n",
        "        ],\n",
        "\n",
        "    ' ass head':\n",
        "        [\n",
        "            'butthead'\n",
        "        ],\n",
        "\n",
        "    ' sex ':\n",
        "        [\n",
        "            'sexy', 's3x', 'sexuality'\n",
        "        ],\n",
        "\n",
        "\n",
        "    ' nigger ':\n",
        "        [\n",
        "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
        "        ],\n",
        "\n",
        "    ' shut the fuck up':\n",
        "        [\n",
        "            'stfu', 'st*u'\n",
        "        ],\n",
        "\n",
        "    ' pussy ':\n",
        "        [\n",
        "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', 'p*ssy'\n",
        "        ],\n",
        "\n",
        "    ' faggot ':\n",
        "        [\n",
        "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
        "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
        "        ],\n",
        "\n",
        "    ' mother fucker':\n",
        "        [\n",
        "            ' motha ', ' motha f', ' mother f', 'motherucker',\n",
        "        ],\n",
        "\n",
        "    ' whore ':\n",
        "        [\n",
        "            'wh\\*\\*\\*', 'w h o r e'\n",
        "        ],\n",
        "    ' fucking ':\n",
        "        [\n",
        "            'f*$%-ing'\n",
        "        ],\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RetJ1vgswOG"
      },
      "source": [
        "def clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
        "\n",
        "  if is_lower:\n",
        "    text=text.lower()\n",
        "    \n",
        "  if remove_patterns_text:\n",
        "    for target, patterns in RE_PATTERNS.items():\n",
        "      for pat in patterns:\n",
        "        text=str(text).replace(pat, target)\n",
        "\n",
        "  if remove_repeat_text:\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
        "\n",
        "  text = str(text).replace(\"\\n\", \" \")\n",
        "  text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "  text = re.sub('[0-9]',\"\",text)\n",
        "  text = re.sub(\" +\", \" \", text)\n",
        "  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
        "  return text "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwyO_GcPuPE_"
      },
      "source": [
        "Cleaning Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cak-Q0fl0qyb"
      },
      "source": [
        "train['comment_text']=train['comment_text'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqRVmH1FRWpL"
      },
      "source": [
        "#### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnhCAXkKUF9i"
      },
      "source": [
        "comments_train=train['comment_text']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwhTNF3pW7Hp"
      },
      "source": [
        "comments_train=list(comments_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuWMc1xqRVqV"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi-vYUcPoi-a"
      },
      "source": [
        "def lemma(text, lemmatization=True):\n",
        "  output=\"\"\n",
        "  if lemmatization:\n",
        "    text=text.split(\" \")\n",
        "    for word in text:\n",
        "       word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n",
        "       word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
        "       word3 = wordnet_lemmatizer.lemmatize(word2, pos = \"a\")\n",
        "       word4 = wordnet_lemmatizer.lemmatize(word3, pos = \"r\")\n",
        "       output=output + \" \" + word4\n",
        "  else:\n",
        "    output=text\n",
        "  \n",
        "  return str(output.strip()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX4DTjGJ-8Id"
      },
      "source": [
        "Lemmatizing Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRquFt30qWk0"
      },
      "source": [
        "lemmatized_train_data = [] \n",
        "\n",
        "for line in tqdm_notebook(comments_train, total=159571): \n",
        "    lemmatized_train_data.append(lemma(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZaP8BH2UG0N"
      },
      "source": [
        "#### Stopwords Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNKjE4Vq8kpG"
      },
      "source": [
        "stopword_list=STOP_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_oqV08mMWmk"
      },
      "source": [
        "Adding Single and Dual to STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go7RVS52I1US"
      },
      "source": [
        "def iter_all_strings():\n",
        "    for size in itertools.count(1):\n",
        "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
        "            yield \"\".join(s)\n",
        "\n",
        "dual_alpha_list=[]\n",
        "for s in iter_all_strings():\n",
        "    dual_alpha_list.append(s)\n",
        "    if s == 'zz':\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_x3bQ4MIGVK"
      },
      "source": [
        "dual_alpha_list.remove('i')\n",
        "dual_alpha_list.remove('a')\n",
        "dual_alpha_list.remove('am')\n",
        "dual_alpha_list.remove('an')\n",
        "dual_alpha_list.remove('as')\n",
        "dual_alpha_list.remove('at')\n",
        "dual_alpha_list.remove('be')\n",
        "dual_alpha_list.remove('by')\n",
        "dual_alpha_list.remove('do')\n",
        "dual_alpha_list.remove('go')\n",
        "dual_alpha_list.remove('he')\n",
        "dual_alpha_list.remove('hi')\n",
        "dual_alpha_list.remove('if')\n",
        "dual_alpha_list.remove('is')\n",
        "dual_alpha_list.remove('in')\n",
        "dual_alpha_list.remove('me')\n",
        "dual_alpha_list.remove('my')\n",
        "dual_alpha_list.remove('no')\n",
        "dual_alpha_list.remove('of')\n",
        "dual_alpha_list.remove('on')\n",
        "dual_alpha_list.remove('or')\n",
        "dual_alpha_list.remove('ok')\n",
        "dual_alpha_list.remove('so')\n",
        "dual_alpha_list.remove('to')\n",
        "dual_alpha_list.remove('up')\n",
        "dual_alpha_list.remove('us')\n",
        "dual_alpha_list.remove('we')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H11kkMtXMyct"
      },
      "source": [
        "for letter in dual_alpha_list:\n",
        "    stopword_list.add(letter)\n",
        "print(\"Done!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti67XuLCNnsL"
      },
      "source": [
        "Checking for other words that we may need in STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4lBqjcBaDVK"
      },
      "source": [
        "def search_stopwords(data, search_stop=True):\n",
        "  output=\"\"\n",
        "  if search_stop:\n",
        "    data=data.split(\" \")\n",
        "    for word in data:\n",
        "      if not word in stopword_list:\n",
        "        output=output+\" \"+word \n",
        "  else:\n",
        "    output=data\n",
        "\n",
        "  return str(output.strip())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn21RMeIbQti"
      },
      "source": [
        "potential_stopwords = [] \n",
        "\n",
        "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
        "    potential_stopwords.append(search_stopwords(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFSkmGjAZuR3"
      },
      "source": [
        "Combining all the sentences in the list into a single string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpmze-Q7BrK0"
      },
      "source": [
        "def string_combine_a(stopword):\n",
        "  final_a=\"\"\n",
        "  for item in range(39893):\n",
        "    final_a=final_a+\" \"+stopword[item]\n",
        "  return final_a\n",
        "\n",
        "def string_combine_b(stopword):\n",
        "  final_b=\"\"\n",
        "  for item in range(39893,79785):\n",
        "    final_b=final_b+\" \"+stopword[item]\n",
        "  return final_b\n",
        "\n",
        "def string_combine_c(stopword):\n",
        "  final_c=\"\"\n",
        "  for item in range(79785,119678):\n",
        "    final_c=final_c+\" \"+stopword[item]\n",
        "  return final_c\n",
        "\n",
        "def string_combine_d(stopword):\n",
        "  final_d=\"\"\n",
        "  for item in range(119678,159571):\n",
        "    final_d=final_d+\" \"+stopword[item]\n",
        "  return final_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqBc4rOahmAz"
      },
      "source": [
        "total_string_potential_a=string_combine_a(potential_stopwords)\n",
        "total_string_potential_b=string_combine_b(potential_stopwords)\n",
        "total_string_potential_c=string_combine_c(potential_stopwords)\n",
        "total_string_potential_d=string_combine_d(potential_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbEm3D8txMSH"
      },
      "source": [
        "Counting the number of words in each of the 4 strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_hhbMS_xR6t"
      },
      "source": [
        "def word_count(str):\n",
        "    counts = dict()\n",
        "    words = str.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in counts:\n",
        "            counts[word] += 1\n",
        "        else:\n",
        "            counts[word] = 1\n",
        "\n",
        "    return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsh0BuT9xSDg"
      },
      "source": [
        "total_string_potential_a_dict=word_count(total_string_potential_a)\n",
        "total_string_potential_b_dict=word_count(total_string_potential_b)\n",
        "total_string_potential_c_dict=word_count(total_string_potential_c)\n",
        "total_string_potential_d_dict=word_count(total_string_potential_d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shmHkLSk3Xxp"
      },
      "source": [
        "Converting Dictionaries to Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSJXHnv6y9T0"
      },
      "source": [
        "total_string_potential_a_df = pd.DataFrame(list(total_string_potential_a_dict.items()),columns = ['Word','Count'])\n",
        "total_string_potential_b_df = pd.DataFrame(list(total_string_potential_b_dict.items()),columns = ['Word','Count'])\n",
        "total_string_potential_c_df = pd.DataFrame(list(total_string_potential_c_dict.items()),columns = ['Word','Count'])\n",
        "total_string_potential_d_df = pd.DataFrame(list(total_string_potential_d_dict.items()),columns = ['Word','Count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjWO0vj2yaW7"
      },
      "source": [
        "Getting Dataframe output in descending order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzThNOjA2pjx"
      },
      "source": [
        "top50_potential_stopwords_a=total_string_potential_a_df.sort_values(by=['Count'],ascending=False).head(50)\n",
        "top50_potential_stopwords_b=total_string_potential_b_df.sort_values(by=['Count'],ascending=False).head(50)\n",
        "top50_potential_stopwords_c=total_string_potential_c_df.sort_values(by=['Count'],ascending=False).head(50)\n",
        "top50_potential_stopwords_d=total_string_potential_d_df.sort_values(by=['Count'],ascending=False).head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1d8reN9CaZ8"
      },
      "source": [
        "Looking for common terms in all top 50 dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKUuEbHvCiYG"
      },
      "source": [
        "common_potential_stopwords=list(reduce(set.intersection,map(set,[top50_potential_stopwords_a.Word,top50_potential_stopwords_b.Word,top50_potential_stopwords_c.Word,top50_potential_stopwords_d.Word])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX3lIQj0l36i"
      },
      "source": [
        "Retaining certain words and removing others from the above list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zacbW5ASjN2r"
      },
      "source": [
        "potential_stopwords=['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-yjjvj3LpZs"
      },
      "source": [
        "Adding above retrived words into the stopwords list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73PbxjxbLvz9"
      },
      "source": [
        "for word in potential_stopwords:\n",
        "    stopword_list.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "101v0iZjaNLR"
      },
      "source": [
        "Removing Stopwords from Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRSga8PKsktA"
      },
      "source": [
        "def remove_stopwords(text, remove_stop=True):\n",
        "  output = \"\"\n",
        "  if remove_stop:\n",
        "    text=text.split(\" \")\n",
        "    for word in text:\n",
        "      if word not in stopword_list:\n",
        "        output=output + \" \" + word\n",
        "  else :\n",
        "    output=text\n",
        "\n",
        "  return str(output.strip())      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-hItiV7skoV"
      },
      "source": [
        "processed_train_data = [] \n",
        "\n",
        "for line in tqdm_notebook(lemmatized_train_data, total=159571): \n",
        "    processed_train_data.append(remove_stopwords(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a29qOSPA-rbS"
      },
      "source": [
        "processed_train_data[152458]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqXHFaSC-Bkf"
      },
      "source": [
        "Removing Stopwords from Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeScalvZDEdD"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkK1P-CdX_0N"
      },
      "source": [
        "max_features=100000      \n",
        "maxpadlen = 200          \n",
        "val_split = 0.3      \n",
        "embedding_dim_fasttext = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIeovLIr6aAo"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(processed_train_data))\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(processed_train_data)\n",
        "word_index=tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "qaA52PlK4xnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfDwmzPj8TJf"
      },
      "source": [
        "Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEmacO337twa"
      },
      "source": [
        "X_t=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtZD1j9BCrsG"
      },
      "source": [
        "indices = np.arange(X_t.shape[0])\n",
        "np.random.shuffle(indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k51hkGqOnZLB"
      },
      "source": [
        "X_t = X_t[indices]\n",
        "labels = y[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWn-SkJCCswd"
      },
      "source": [
        "### Splitting data into Training and Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5hOSGJVb7o4"
      },
      "source": [
        "num_validation_samples = int(val_split*X_t.shape[0])\n",
        "x_train = X_t[: -num_validation_samples]\n",
        "y_train = labels[: -num_validation_samples]\n",
        "x_val = X_t[-num_validation_samples: ]\n",
        "y_val = labels[-num_validation_samples: ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL7Mv1Z6D8_w"
      },
      "source": [
        "### Importing Fast Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsvNLXZ6DlWS"
      },
      "source": [
        "##PLEASE IMPORT THIS FROM https://fasttext.cc/docs/en/english-vectors.html in your working directory or from SFU Vault: https://vault.sfu.ca/index.php/s/AEvM2fabFwXTSZZ\n",
        "embeddings_index_fasttext = {}\n",
        "f = open('wiki-news-300d-1M.vec', encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpkSOB6iEFCB"
      },
      "source": [
        "embedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index_fasttext.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_fasttext[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OoZ7fTpPWhi"
      },
      "source": [
        "### Creating Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwzg_WX32OG3"
      },
      "source": [
        "#### Talos Grid Search  for LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZdik3-jGkye"
      },
      "source": [
        "def toxic_classifier(x_train,y_train,x_val,y_val,params):\n",
        "\n",
        "  inp=Input(shape=(maxpadlen, ),dtype='int32')\n",
        "\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                           embedding_dim_fasttext,\n",
        "                           weights = [embedding_matrix_fasttext],\n",
        "                           input_length = maxpadlen,\n",
        "                           trainable=False,\n",
        "                           name = 'embeddings')\n",
        "  embedded_sequences = embedding_layer(inp)\n",
        "\n",
        "  x = LSTM(params['output_count_lstm'], return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
        "  x = GlobalMaxPool1D()(x)\n",
        "  x = Dropout(params['dropout'])(x)\n",
        "  x = Dense(params['output_count_dense'], activation=params['activation'], kernel_initializer='he_uniform')(x)\n",
        "  x = Dropout(params['dropout'])(x)\n",
        "  preds = Dense(6, activation=params['last_activation'], kernel_initializer='glorot_uniform')(x)\n",
        "  model = Model(inputs=inp, outputs=preds)\n",
        "  model.compile(loss=params['loss'], optimizer=params['optimizer'], metrics=['accuracy'])\n",
        "  model_info=model.fit(x_train,y_train, epochs=params['epochs'], batch_size=params['batch_size'],  validation_data=(x_val, y_val))\n",
        "\n",
        "  return model_info, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6AtSibHGoTU"
      },
      "source": [
        "p={\n",
        "    'output_count_lstm': [40,50,60],\n",
        "    'output_count_dense': [30,40,50],\n",
        "    'batch_size': [32],\n",
        "    'epochs':[2],\n",
        "    'optimizer':['adam'],\n",
        "    'activation':['relu'],\n",
        "    'last_activation': ['sigmoid'],\n",
        "    'dropout':[0.1,0.2],\n",
        "    'loss': ['binary_crossentropy']   \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAnaHxh1GpFi"
      },
      "source": [
        "scan_results = talos.Scan(x=x_train,\n",
        "               y=y_train,\n",
        "               x_val=x_val,\n",
        "               y_val=y_val,\n",
        "               model=toxic_classifier,\n",
        "               params=p,\n",
        "               experiment_name='tcc',\n",
        "               print_params=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXl1ZQSWtdYK"
      },
      "source": [
        "model_id = scan_results.data['val_accuracy'].astype('float').argmax()\n",
        "model_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yict0QyGxV7"
      },
      "source": [
        "analyze_object = talos.Analyze(scan_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3d78iYYG0I3"
      },
      "source": [
        "analyze_object.best_params('val_accuracy', ['accuracy', 'loss', 'val_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHrPeYI_Giky"
      },
      "source": [
        "#### Training Model with Best Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIRoKKp9mzQz"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x04BXTCCdsV"
      },
      "source": [
        "inp=Input(shape=(maxpadlen, ),dtype='int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-fPTQKHFF61"
      },
      "source": [
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                           embedding_dim_fasttext,\n",
        "                           weights = [embedding_matrix_fasttext],\n",
        "                           input_length = maxpadlen,\n",
        "                           trainable=False,\n",
        "                           name = 'embeddings')\n",
        "embedded_sequences = embedding_layer(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAF1Ui22FmHX"
      },
      "source": [
        "#Select Model with Best Parameters from above Talos.scan \n",
        "x = LSTM(50, return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(60, activation=\"relu\", kernel_initializer='he_uniform')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "preds = Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kigFyK4cHHrv"
      },
      "source": [
        "model_1 = Model(inputs=inp, outputs=preds)\n",
        "model_1.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.AUC(),tf.keras.metrics.Recall()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_qJGa25HVxi"
      },
      "source": [
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSqW0thULSae"
      },
      "source": [
        "model_info_1=model_1.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wv1IdwZk_qG"
      },
      "source": [
        "## Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eisFdjOUlBbM"
      },
      "source": [
        "#Model file accesible from https://vault.sfu.ca/index.php/apps/files/?dir=/&fileid=110815034\n",
        "model_1.save(filepath=\"Model_LSTM_RNN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXx4IOYn4_XB"
      },
      "source": [
        "## Loading Saved Model and Print Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huVktHqRMSGc"
      },
      "source": [
        "loaded_model_1 = keras.models.load_model(filepath=\"Model_LSTM_RNN\")\n",
        "metrics =loaded_model_1.evaluate(x_val, y_val, return_dict=True)\n",
        "print(str(metrics))\n",
        "fscore = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])\n",
        "print('F-Score: ' + str(fscore))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "cmpt413-project-final-notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1tkZEcx61dvK",
        "D8hAcigh3Sh3",
        "6z-4hq4jyhBw",
        "mqRVmH1FRWpL",
        "yZaP8BH2UG0N"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}